# Data Processing Workflow

We are provided with the video (Youtube links) and audio (in mp3 format) extracted from the video.  The transcript of courtroom proceedings, prepared by [TERES](https://teres.ai/) is provided in a pdf format. The transcript has proper speaker annotation, but no timestamps.

The task is to preprocess the speech and audio data into a format suitable for ASR training. This involves aligning the speech data with the transcript so that the audio can be sliced into smaller chunks <= 20s with associated transcripts.

The minimal requirement is to have a dataset in the following format



| Filepath   | Transcript                                                 |
| ---------- | ---------------------------------------------------------- |
| audio1.wav | That can be postponed, their putting it up on the website. |
| audio2.wav | Fair enough. No objection                                  |

A better processing pipeline would be to include some additional information like (For later):\
\


<table><thead><tr><th>Filepath</th><th>Speaker</th><th>Transcript</th><th data-type="number">Duration (s)</th></tr></thead><tbody><tr><td>audio1.wav</td><td>JDYC</td><td>That can be postponed, their putting it up on the website.</td><td>2.3</td></tr><tr><td>audio2.wav</td><td>KS</td><td>Fair Enough. No objection</td><td>1.1</td></tr></tbody></table>



### Text Processing Pipeline

The proceedings report generated by TERES is in pdf format. The transcript text has to be extracted with or without speaker information. Casing and punctuation has to be retained, even if it may not be used during trainings and evaluation of the court room ASR system.

### Audio Processing Pipeline

The ultimate aim is to slice the audio with matching transcripts. This involves a [Forced Alignment](data-processing-workflow.md#text-processing-pipeline) (FA) step where audio and transcript are aligned with time stamps. Slicing the audio can be done once you have proper time stamps.

#### Forced Alignment

FA requires an already trained fairly good ASR system for the language and domain under consideration. Considering the reasonably good quality of Meta's [MMS](https://ai.meta.com/blog/multilingual-model-speech-recognition/) on speech recognition than Whisper (atleast as per their claim), I used an actively maintained forced aligner ([https://github.com/MahmoudAshraf97/ctc-forced-aligner](https://github.com/MahmoudAshraf97/ctc-forced-aligner)) under BSD License. The original MMS is CC-BY-NC 4.0, not suitable for commercial use.

#### Audio Slicer

Once you have the time stamps it can be easily converted to SRT format of subtitles. I had a 5 year old [side-project](https://github.com/kavyamanohar/audioslicer) that created audio chunks based on information from the srt files. It works out-of-the box, but planning to modernize it for this assignment.

### Challenges

1. The CTC-Forced-Aligner provides timestamps at different granularity (characters, words and sentences). For our use case it would be the best to use sentences. The tool uses NLTK  for sentence splitting, but the splitting happens after abbreviations like Mr.&#x20;
2. Unlike popular read speech datasets, real court-room conversations contain very small spoken sentences like 'That's right' which takes less than 1s duration. Such small audio slices are not suitable for ASR training \[citation to be added]. Such small audios should be dropped from training.

